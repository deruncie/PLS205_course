---
title: "Lab 9 - Incomplete designs"
author: Daniel Runcie. Modified from labs designed by Iago Hale, Dario Cantu and Jorge
  Dubcovsky
output: 
  html_notebook: 
    toc: true 
    toc_float: true 
---

```{r echo = F,warning=FALSE,message=FALSE}
library(lmerTest)
library(emmeans) 
library(pbkrtest)
library(ggplot2)
```



## Incomplete block design
The analysis of incomplete block designs is very similar to the analysis of complete block designs. 
You still need to specify a model with all blocking terms, declare any random terms, and ask for specific contrasts using `emmeans`.

There are only two main differences between analyses of complete and incomplete block designs:

1. The order of terms in the model matters for incomplete block designs. **always put blocking factors first, treatments last**
2. Declaring blocks **random** does affect the analysis of incomplete blocks, even if there are no replicates of any treatment in each block

As an example, we'll analyze data from a classic experiment investigating soybean yield in Ames, Iowa in 1937.
The experiment consisted of 31 varieties tested in a single field, which was divided into 31 incomplete blocks.
Each block contained 6 plots.

```{r}
soybeans = read.csv('soybean.csv',colClasses = c('factor','factor','numeric'))  # you can directly assign column classes (factor / numeric) when you load the data if you know what the data will look like
str(soybeans)
```

### Visualsing the design
With a blocked design, a table that counts the number of occurences of each treatment in each block 
can help visualize the experiment. The `table` function counts the number of occurences of each combination of the listed factors.
This is called an **incidence matrix** for the design
```{r}
incidence_mat = table(soybeans$block,soybeans$gen)
incidence_mat
```

We can also plot table table with the `plot()` function:
```{r,fig.width=10,fig.height=10}
plot(incidence_mat,las=2)
```
**Note**: I expanded the size of the figure by adding `{r,fig.width=10,fig.height=10}` to the beginning of the code chunk

### Inspecting balance
One use of the incidence matrix is to calculate the number of times each pair of treatments occurs together
in the same block. This is easily calculated from the incidence matrix like this:
```{r}
crossprod(incidence_mat)
```
This table counts the number of times each pair of genotypes (columns of `incidence_mat`) occurs together in the same block (rows of `incidence_mat`).

> As you can see from this matrix, the design is balanced, with lambda=1. Each pair of treatments occurs in the same block exactly once.

### Consequences of lost blocks
To explore these functions, say Blocks `B01`, `B02` and `B03` were lost. Look at the incidence matrix for these blocks:
```{r}
incidence_mat[1:3,]
```

Using `crossprod`, we can see how this would affect the balance of the whole experiment

```{r}
soybeans_lostBlocks = subset(soybeans,block %in% c('B01','B02','B03') == F)
incidence_mat_lost_blocks = table(soybeans_lostBlocks$block,soybeans_lostBlocks$gen)
crossprod(incidence_mat_lost_blocks)
```

> From this, we can see that the design is no-longer balanced. Some pairs of treatments do not occur in the same block. 
Also, some treatments only have 4 or 5 reps. With this subset of the data, the precisions of estimates of treatment means and pairwise contrasts
will not be all the same.

### Analysis of incomplete blocks

#### Fixed blocks
The main consequence of incomplete blocks for analysis is that the order of terms in the linear model matters, 
at least for ANOVA, and when blocks are fixed.

The following code compares two models, one with `gen` listed first, and the other with `block` listed first.
```{r}
lm_genFirst = lm(yield ~ gen + block,soybeans)
anova(lm_genFirst)
```

```{r}
lm_blockFirst = lm(yield ~ block + gen,soybeans)
anova(lm_blockFirst)
```

#### Questions
> Compare the F-values for gen in the two models. Which model identifies larger differences among genotypes?

> Compare the Residuals line. Do the models differ in how well they can explain the data?

Since the models provide exactly the same fits to the data, we cannot use that to decide which model is correct. 
Notice that the first one assigns more Sum Sq to `gen`, and less Sum Sq to 'block' than the second model. This makes the first
more optimistic about the effect of `gen` than the second.
To be conservative, we'll use the second model to test the null hypothesis that all genotypes are equivalent.

#### Random blocks
Unlike with a basic RCBD (without reps), for an incomplete block design, declaring blocks to be random 
will change the results (though usually not that much). 
```{r}
blocks_random = lmer(yield ~ (1|block) + gen,soybeans)
anova(blocks_random,ddf='K')
```

With many blocks, few treatments per block, and small block differences, this random model will tend to be more powerful
than the fixed model (ie larger F.value, smaller confidence intervals).

### Comparing means
Means comparisions in incomplete block designs proceed identically to those for complete blocks.
I'll demonstrate for the fixed-blocks model.
Since there are many genotypes, I'll focus on comparisons among just the first three:
```{r}
trt_means = emmeans(lm_blockFirst,specs = 'gen',at=list(gen = c('G01','G02','G03')))
contrasts = contrast(trt_means,method = 'pairwise')
summary(contrasts,infer = c(T,F))
```

### Excercise:
Repeate the above exercises with the following dataset that lost the first three blocks.

Look specifically at the comparisons between G03, G06, G07, G11, and G12.
Try to explain why each difference among genotypes has a different standard error.
```{r}
soybean_missingBlocks = subset(soybeans,block %in% c('B01','B02','B03') == F)

# anova
# means comparisons. Use at=list(gen = c('G03','G06','G07','G11','G12')) to specifify only these genotypes.
```

> Are all standard errors among contrasts the same? Compare the standard errors on the G03-G06, G03-G07, and G11-G12 contrasts.



-------------------------------

## Unconnected design
Blocked designs do not need to be complete, or balanced. Balanced designs are often preferred because
all treatments are treated equally. But lack of balance doesn't inherently bias an experiment - it 
just makes standard errors larger for some contrasts than others.

However, a more serious problem in an experimental design is a lack of **connectedness**.
Two treatments are not connected if they are not present in the same block, and there is no other
treatment (or treatments) that can serve as a bridge between the first two treatments going through other blocks.

Consider the following dataset:
```{r}
unconnected_data = read.csv('unconnected.csv',colClasses = c('factor','factor'))
str(unconnected_data)
```

We can visualize the design with the table function

```{r}
incidence_mat <- table(unconnected_data$Block,unconnected_data$Trt)
incidence_mat
```

> Can you construct an estimator for A-B?

We could estimate A-C from blocks 1 and 3, and B-D from blocks 2 and 4. But there's no way to **connect** A and B
with estimates taken from inside the same block.

In general, testing connectedness is difficult. Designs can be connected through multi-step chains: A-D = (A-B) - (C-B) - (D-C)

The easier way to test is to use emmeans to test if the contrast could be estimated. Do do this, we can take 
our data.table with our design, and an some fake data.
```{r}
unconnected_data$Fake_y = rnorm(nrow(unconnected_data))  # This makes some fake data
```

Now, analyze your experiment using the fake data as you normally would:
```{r}
unconnected_model = lm(Fake_y~Block+Trt,unconnected_data)
unconnected_means = emmeans(unconnected_model,specs = 'Trt')
unconnected_effects = contrast(unconnected_means,method='pairwise')
summary(unconnected_effects)
```

emmeans tells us that it can estimate A-C and B-D, but none of the other contrasts. So this would not be
a good experiment if you wanted to study A-B!

-------------------------------------

## Augmented design

An augmented design can be used when a large number of treatments need to be compared to one or more controls.

This design uses reduced replication of the test treatments to keep block sizes manageable. The design 
is not very good for making comparisons among test treatments, but is OK for comparing each test treatment to the control(s).

In the following dataset, 34 new variaties are tested against 4 controls. An Augmented RCBD is used, with 6 blocks

```{r}
augblock <- read.csv('augmented_blocks.csv')
str(augblock)
```

We can check the variety names to identify the control (also called "Check" varieties)
```{r}
levels(augblock$Var)
```

Clearly, the first 4 varieties are the checks.

### Design table

**Design**: Augmented RCBD with 4 controls

| Structure | Variable | Type        | # levels | Experiment Unit |
|-----------|----------|-------------|----------|-----------------|
| Treatment | Var      | Categorical | 38       | Plot            |
| Design    | Block    | Categorical | 6        |                 |
|           | Plot     | Categorical | 78       |                 |
| Response  | gw       | Numeric     | 78       |                 |

### Visualizing the design

Use the `table` function to visualize the design
```{r}
table(augblock$Block,augblock$Var)
```

### Questions:
> Are any of the test varieties in multiple blocks? What would happen if a test variety was put in multiple blocks?
> Are any of the test varieties replicated? How does this change the analysis?
> Are the blocks complete for the controls? Is it necessary that all the controls be in each block?

### Analysis:
The analysis itself proceeds as normal:

Fit the model. Make sure to put Block first

```{r}
augmented_model <- lm(gw ~ Block + Var,augblock)
```

Use an ANOVA to test for differences among varieties
```{r}
anova(augmented_model)
```

> We don't find any evidence in the ANOVA any of the Varieties differ from eachother. Should we stop here?

In this case, maybe not. The ANOVA test on Var asks if ANY variety differs from anyother (like the Tukey test).
This is more conservative than we need to be, because we are only interested in tests against the controls.

To test the varieties against the controls, we use emmeans, where we use `ref=1:4` to specify that our "control" is the average of 4 varieties.
```{r}
augmented_means <- emmeans(augmented_model,specs = 'Var')
augmented_effects <- contrast(augmented_means,method='trt.vs.ctrl',ref = 1:4)
summary(augmented_effects)
```

> With the increased power, we can identify only a single variety (IC-03688) with lower gw than the average of the 4 controls at alpha = 0.05.

> However, Our goal here is really to do a screening of this large number of treatments efficiently. 
Maybe using alpha = 0.05 is too stringent. It would probably be reasonable to take several of the varieties with the most extreme 
estimates to use in a more-focused follow-up experiment.

